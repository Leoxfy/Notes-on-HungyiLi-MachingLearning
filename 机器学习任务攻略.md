## 机器学习任务攻略

![4](E:\TO PKU\计算机相关\李宏毅机器学习笔记\asserts\4.png)

我写了resnet才知道，56层的网络训练error不够好不是因为过拟合的问题，是优化方法不对，我写了resnet才知道，identity的意义，就是后面几层 哪怕不学东西也一定可以 获得前面layer的梯度，所以residual块的作用还是 解决优化的问题 或者 更细一点 让更深的网络更好训练。当时何凯明也没有意识到这个问题2015年的时候，7年过去了，人们发现其实resnet解决的还是训练的问题更形象地说56层的网络弹性一定比20层的网络要好。

![5](E:\TO PKU\计算机相关\李宏毅机器学习笔记\asserts\5.png)

所以这种就属于优化器的问题了，只有training loss小 test loss大才是overfitting。

![3](E:\TO PKU\计算机相关\李宏毅机器学习笔记\asserts\3.png)

所以其实我们说泛化性能好，本质是在说 模型(函数) 能够拟合数据的真实分布，表面是说 对于新来的数据 没有见过的数据 模型能预测出一个正确的结果。本质还是看模型拟合的分布。

![1](E:\TO PKU\计算机相关\李宏毅机器学习笔记\asserts\1.png)

![2](E:\TO PKU\计算机相关\李宏毅机器学习笔记\asserts\2.png)

这个图的意思是 训练数据更多 对于模型拟合真实数据分布的约束性越强 而数据增强又是扩充训练集的有效方法 但是需要注意的是——不要生成一些真实世界不存在的数据 因为这些数据不符合 真实数据的分布。

![3](E:\TO PKU\计算机相关\李宏毅机器学习笔记\asserts\3.png)



![13](E:\TO PKU\计算机相关\李宏毅机器学习笔记\asserts\13.png)

为什么cnn图像做的比fc要好呢？ 因为卷积这个操作子实际上是对模型做了一些constraint （也算是一种正则）所以可以防止fc的overfitting的问题。

## 局部最小值与鞍点

![6](E:\TO PKU\计算机相关\李宏毅机器学习笔记\asserts\6.png)

![7](E:\TO PKU\计算机相关\李宏毅机器学习笔记\asserts\7.png)

hessian矩阵判断鞍点 极大 极小点

![8](E:\TO PKU\计算机相关\李宏毅机器学习笔记\asserts\8.png)

所以这张slide告诉我们 其实在高维空间里局部最小点是很少见的 更多的情况是卡在鞍点的位置 直观上理解 就是一个函数 在低维度空间看是最小点 但在高维空间是鞍点。

![9](E:\TO PKU\计算机相关\李宏毅机器学习笔记\asserts\9.png)

比如左边的两幅图

## batch和momentum

![10](E:\TO PKU\计算机相关\李宏毅机器学习笔记\asserts\10.png)

batch size选择的智慧

![11](E:\TO PKU\计算机相关\李宏毅机器学习笔记\asserts\11.png)

![12](E:\TO PKU\计算机相关\李宏毅机器学习笔记\asserts\12.png)

关于好的minima和 坏的minima。所以虽然服务器显存很大 但还是小batch size比较好。小batch更容易出现好的minima。